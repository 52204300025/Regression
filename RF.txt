# -!- coding: utf-8 -!-
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.metrics import median_absolute_error as mae
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn import metrics
from sklearn.metrics import r2_score
import os
os.chdir('')
df=pd.read_excel('1-Octene.xlsx')

df_cor=df.corr()
fig1 = plt.figure(figsize=(10,9), dpi=50)
sns.heatmap(data=df_cor,cmap="YlGnBu",robust=True,annot=True)
plt.title('Correlation matrix',fontsize=15)
plt.show()

X=df.iloc[:,0:8].values
y=df.iloc[:,9].values
print(df.head())


#划分数据集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=90)
print("----grid search")
param_test = {'n_estimators': range(100, 300, 10), 'max_depth': range(10, 30, 1), 'min_samples_split': range(2,10, 1)}
print(param_test)
gsearch = GridSearchCV(estimator=RandomForestRegressor(min_samples_leaf=1, max_features='sqrt',
                        random_state=90, oob_score=True), param_grid=param_test, scoring='r2', cv=5)
gsearch.fit(X_train, y_train)
gsearch.best_params_, gsearch.best_score_
best_n_estimators = gsearch.best_params_['n_estimators']
best_max_depth = gsearch.best_params_['max_depth']
best_min_samples_split = gsearch.best_params_['min_samples_split']
print("best_n_estimators:", best_n_estimators)
print("best_max_depth:", best_max_depth)
print("best_min_samples_split:", best_min_samples_split)
print('best_n_estimators score:', gsearch.best_score_)
print('CV:', gsearch.cv_results_)

# 把最优参数全部获取去做随机森林拟合
best_model = RandomForestRegressor(n_estimators=best_n_estimators, max_depth=best_max_depth,
                            min_samples_leaf=1, min_samples_split=best_min_samples_split,
                            max_features='sqrt', random_state=90, oob_score=True)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_train)
y2_pred = best_model.predict(X_test)


#R2——评价指标，此段为训练集，直接套用评价指标函数，必须要返回模型
def R22_0(estimator,X_train, y_train):
    y_pred=estimator.predict(X_train)
    SStot=np.sum((y_train-np.mean(y_train))**2)
    SSres=np.sum((y_train-y2_pred)**2)
    r2=1-SSres/SStot
    return r22

def R2_0(estimator,X_test,y_test):
    y2_pred=estimator.predict(X_test)
    SStot=np.sum((y_test-np.mean(y_test))**2)
    SSres=np.sum((y_test-y2_pred)**2)
    r2=1-SSres/SStot
    return r2

print('r2(training sets)):', r2_score(y_train, y_pred))
print('r2(test sets):', r2_score(y_test, y2_pred))
print('Mae(test sets):', metrics.mean_absolute_error(y_test, y2_pred))
print('Mse(test sets):', metrics.mean_squared_error(y_test, y2_pred))
print('RMse(test sets):',
      np.sqrt(metrics.mean_squared_error(y_test, y2_pred)))


#重要性分析函数
importances = list(best_model.feature_importances_)
print(importances)
fig2=plt.figure(dpi=500)
feature_list = list(df.columns)[0:8]
feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]
feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)
x_values = list(range(len(importances)))
print(x_values)
plt.bar(x_values, importances, orientation = 'vertical')
plt.xticks(x_values, feature_list,rotation=0)
plt.ylabel('%'); plt.xlabel('feature'); plt.title('Descriptor importance');
plt.show()

#绘制回归图——画布
fig3=plt.figure(dpi=500)
import seaborn as sns
sns.regplot(x=y_train, y=y_pred)
plt.xlabel('predictions')
plt.ylabel('actual value')
plt.title('Training sets performance plots')
plt.show()

#绘制回归图——画布
fig4=plt.figure(dpi=500)
import seaborn as sns
sns.regplot(x=y_test, y=y2_pred)
plt.xlabel('predictions')
plt.ylabel('actual value')
plt.title('Test sets performance plots')
plt.show()

